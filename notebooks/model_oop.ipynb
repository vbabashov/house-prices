{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# This script pulls in data, builds and tests several predictive models,and then makes predictions on test data using the best model.'''\n",
    "\n",
    "__author__ = \"Vusal Babashov\"\n",
    "__email__ = \"vbabashov@gmail.com\"\n",
    "__website__ = 'https://vbabashov.github.io'\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import math \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, KFold, cross_validate\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor \n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "from feature_engine import encoding as enc #RareLabelEncoder, OrdinalEncoder\n",
    "from feature_engine.selection import SelectByTargetMeanPerformance, DropFeatures \n",
    "from feature_engine.creation import CombineWithReferenceFeature, MathematicalCombination\n",
    "\n",
    "\n",
    "class Data:\n",
    "    '''create train and test dataframe'''\n",
    "    def __init__(self, train_file, test_file, nom_cols, ord_cols, ordinal_categories_list, num_cols, target_col):\n",
    "        #create new copies instead of references\n",
    "        self.nom_cols = list(nom_cols)\n",
    "        self.ord_cols = list(ord_cols)\n",
    "        self.num_cols = list(num_cols)\n",
    "        self.target_col = target_col\n",
    "        self.ord_cat_list = list(ordinal_categories_list)\n",
    "        self.cat_cols = self.nom_cols + self.ord_cols\n",
    "        self.feature_cols = self.cat_cols + self.num_cols + self.ord_cols\n",
    "        self.cols_to_drop =[]\n",
    "        self.ordinal_encoders = {}\n",
    "        self.train_df = self._create_train_df(train_file)\n",
    "        self.test_df  = self._create_test_df(test_file)\n",
    "        self.encoder = None\n",
    "\n",
    "\n",
    "    def _load_data(self,file):\n",
    "        '''loads csv to pd dataframe'''\n",
    "        return pd.read_csv(file)\n",
    "\n",
    "\n",
    "    def _log_transform (self, df):\n",
    "        '''This function performs the log transformation of the target'''\n",
    "        df['SalePrice'] = np.log(df['SalePrice'])\n",
    "        return df\n",
    "\n",
    "\n",
    "    def _drop_missing_cols (self,df):\n",
    "        '''Identifies and drops the columns with 80% or hihgher proportion of missing data '''\n",
    "        dropped_cols = []  \n",
    "        for col in df.columns:\n",
    "            if df[col].isnull().sum()/df.shape[0] >= 0.8:\n",
    "                dropped_cols.append(col)\n",
    "        df.drop(columns=dropped_cols, inplace=True)\n",
    "        return df, dropped_cols \n",
    "\n",
    "\n",
    "    def _impute_missing_values (self, df, categorical_features, numeric_features):\n",
    "        ''' Imputes the continious columns with median and categorical columns with the mode value'''\n",
    "        imputer_con = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "        imputer_cat = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "        for col in categorical_features+numeric_features:\n",
    "            if df[col].isnull().sum() > 0:    \n",
    "                if col in categorical_features:              \n",
    "                    df[col] = imputer_cat.fit_transform(df[col].values.reshape(-1,1))\n",
    "                elif col in numeric_features:  \n",
    "                    df[col] = imputer_con.fit_transform(df[col].values.reshape(-1,1))\n",
    "        return df \n",
    "\n",
    "\n",
    "    def _ordinal_encode (self, df, ord_cols, ordinal_categories_list):\n",
    "        '''This function encodes ordinal variables into ordinal encoding and combines wit the rest of the dataframe'''\n",
    "        encoder = OrdinalEncoder(categories=ordinal_categories_list)\n",
    "        df[ord_cols] = encoder.fit_transform(df[ord_cols])\n",
    "        return df\n",
    "    \n",
    "    \n",
    "    def _inverse_ordinal_encode (self, df, ord_cols):\n",
    "         df[ord_cols] = self.encoder.inverse_transform(df[ord_cols]) \n",
    "          \n",
    "    \n",
    "    def _create_train_df(self, train_file, preprocess=True):\n",
    "        '''loads and encodes train data'''\n",
    "        train_df = self._load_data(train_file)\n",
    "        if preprocess:\n",
    "            train_df = self._log_transform(train_df)\n",
    "            train_df, self.cols_to_drop = self._drop_missing_cols(train_df)\n",
    "            train_df = self._impute_missing_values(train_df, self.cat_cols, self.num_cols)\n",
    "            train_df = self._convert_month_string(train_df)\n",
    "            train_df = self._ordinal_encode(train_df, self.ord_cols, self.ord_cat_list)\n",
    "            train_df = self._convert_data_types(train_df)\n",
    "        return train_df\n",
    "\n",
    "    \n",
    "    def _create_test_df (self,test_file, preprocess=True):\n",
    "        '''loads and ordinal encodes test data'''\n",
    "        test_df = self._load_data(test_file)\n",
    "        if preprocess:\n",
    "            test_df = test_df.drop(columns=self.cols_to_drop, axis=1)\n",
    "            test_df = self._impute_missing_values(test_df, self.cat_cols, self.num_cols)\n",
    "            test_df = self._convert_month_string(test_df)\n",
    "            test_df = self._ordinal_encode(test_df, self.ord_cols, self.ord_cat_list) \n",
    "            test_df = self._convert_data_types(test_df)\n",
    "        return test_df\n",
    "           \n",
    "\n",
    "    def _convert_month_string (self, df):\n",
    "        '''This function maps the numerical month names into string month names'''\n",
    "        d = { 1 : 'Jan',\n",
    "              2 : 'Feb',\n",
    "              3 : 'Mar',\n",
    "              4 : 'Apr',\n",
    "              5 : 'May',\n",
    "              6 : 'June',\n",
    "              7 : 'July',\n",
    "              8 : 'Aug',\n",
    "              9 : 'Sep',\n",
    "              10: 'Oct',\n",
    "              11: 'Nov',\n",
    "              12: 'Dec'\n",
    "        }\n",
    "        df['MoSold'] = df ['MoSold'].map(d)\n",
    "        return df\n",
    "\n",
    "\n",
    "    def _convert_data_types (self, df):\n",
    "        '''This function coverts the categorical variables into object and numeric variables into int types'''\n",
    "        df[self.nom_cols] = df[self.nom_cols].astype('O')\n",
    "        df[self.ord_cols] = df[self.ord_cols].astype('int')\n",
    "        df[self.num_cols] = df[self.num_cols].astype('int') \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureGenerator:\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        '''creates feature generator pipeline'''\n",
    "        self.data = data\n",
    "        self.cat_cols = data.cat_cols\n",
    "        self.pipeline = None\n",
    "    \n",
    "    def create_pipeline (self, features, target):\n",
    "        ''' Creates engineer various features using pipeline'''\n",
    "        rare_encoder = enc.RareLabelEncoder(tol = 0.05, n_categories=4, variables = data.nom_cols)\n",
    "        price_encoder = enc.OrdinalEncoder (encoding_method='ordered',  variables = data.nom_cols)\n",
    "\n",
    "        age = CombineWithReferenceFeature(\n",
    "            variables_to_combine=['YrSold'],\n",
    "            reference_variables=['YearBuilt', 'YearRemodAdd', 'GarageYrBlt'],\n",
    "            operations = ['sub']\n",
    "        )     \n",
    "\n",
    "        bath = MathematicalCombination(\n",
    "            variables_to_combine=['BsmtHalfBath', 'BsmtFullBath', 'FullBath', 'HalfBath'],\n",
    "            math_operations=['sum'],\n",
    "            new_variables_names=['TotalBath'],\n",
    "        )\n",
    "\n",
    "        area = MathematicalCombination(\n",
    "            variables_to_combine=['1stFlrSF', '2ndFlrSF', 'TotalBsmtSF'],\n",
    "            math_operations=['sum'],\n",
    "            new_variables_names=['TotalArea'],\n",
    "        )\n",
    "\n",
    "        drop = DropFeatures(\n",
    "            features_to_drop=['YearBuilt','YrSold','YearRemodAdd', 'GarageYrBlt']\n",
    "        )\n",
    "\n",
    "        pipe = Pipeline(steps=[ \n",
    "                                ('rare_encoder', rare_encoder), \n",
    "                                ('ordinal_encoder', price_encoder),\n",
    "                                ('cobinator',age),\n",
    "                                ('bath', bath),\n",
    "                                ('area', area),\n",
    "                                ('drop', drop)\n",
    "                            ])\n",
    "        self.pipeline = pipe.fit(features, target)  \n",
    "    \n",
    "    def create_features(self,df):        \n",
    "        '''Transform data and Generate Features'''\n",
    "        df = self.pipeline.transform (df)\n",
    "        return df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelContainer:\n",
    "    \n",
    "    def __init__(self):\n",
    "        '''initializes model list and dicts'''\n",
    "        self.best_algorithm  = None\n",
    "        self.gridcvs = {}\n",
    "        self.scores_dict = None\n",
    "        self.best_model = None\n",
    "        self.best_params  = None\n",
    "        self.best_score  = 0\n",
    "        self.predictions = None\n",
    "        self.train_mae = 0\n",
    "        self.test_mae  = 0\n",
    "        self.train_r2  = 0\n",
    "        self.test_r2   = 0\n",
    "        self.mean_rmse = {}\n",
    "    \n",
    "        \n",
    "    def nested_cross_validation(self, features, target):\n",
    "        '''This function performs the nested 5x2cv procedure and selects best algorithm'''\n",
    "         \n",
    "        self.gridcvs = {}\n",
    "            \n",
    "        reg1 = RandomForestRegressor(random_state=1)\n",
    "        reg2 = XGBRegressor(random_state=1)\n",
    "        reg3 = LGBMRegressor(random_state=1)   \n",
    "        \n",
    "            \n",
    "        param_grid1 = {'n_estimators': [500,1000]}\n",
    "        \n",
    "        param_grid2 = { 'colsample_bytree':[0.6, 0.8], \n",
    "                         'max_depth': [8,10],\n",
    "                         'min_child_weight':[3,7], \n",
    "                         'regsubsample' :[0.6, 0.8]}\n",
    "        \n",
    "        param_grid3 = {'num_leaves': [6, 8, 20, 30],\n",
    "                        'max_depth': [2, 4, 6, 8, 10],\n",
    "                        'n_estimators': [50, 100, 200, 500],\n",
    "                        'colsample_bytree': [0.3, 1.0]}\n",
    "\n",
    "    \n",
    "        inner_cv = KFold(n_splits=2, shuffle=True, random_state=1)\n",
    "\n",
    "        for pgrid, est, name in zip((param_grid1, param_grid2, param_grid3),\n",
    "                                    (reg1, reg2, reg3),\n",
    "                                    ('RForest','Xgboost', 'LightGBM')):\n",
    "\n",
    "            gcv = GridSearchCV(estimator=est,\n",
    "                               param_grid=pgrid,\n",
    "                               scoring = 'neg_root_mean_squared_error',\n",
    "                               n_jobs=-1,\n",
    "                               cv=inner_cv,\n",
    "                               verbose=0,\n",
    "                               refit=True)\n",
    "            self.gridcvs[name] = gcv\n",
    "\n",
    "\n",
    "        outer_cv = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    \n",
    "        for name, gs_est in sorted(gridcvs.items()):\n",
    "            self.scores_dict = cross_validate(gs_est, \n",
    "                                         X=features, \n",
    "                                         y=target,\n",
    "                                         verbose=0,\n",
    "                                         cv=outer_cv,\n",
    "                                         return_estimator=True,\n",
    "                                         n_jobs=-1\n",
    "                                        )\n",
    "\n",
    "            print(50 * '-', '\\n')\n",
    "            print('Algorithm:', name)\n",
    "            print('    Inner loop:')\n",
    "\n",
    "\n",
    "            for i in range(self.scores_dict['test_score'].shape[0]):\n",
    "                print('\\n      Best RMSE Score (avg. of inner test folds) %.2f' % np.absolute(self.scores_dict['estimator'][i].best_score_))\n",
    "                print('        Best parameters:', self.scores_dict['estimator'][i].best_estimator_)\n",
    "                print('        RMSE Score (on outer test fold) %.2f' % np.absolute(self.scores_dict['test_score'][i]))\n",
    "            print('\\n%s |  outer test folds Ave. Score %.2f +/- %.2f' % \n",
    "                                  (name, np.absolute(self.scores_dict['test_score']).mean(), \n",
    "                                   np.absolute(self.scores_dict['test_score']).std()))    \n",
    "            \n",
    "            self.mean_rmse[gs_est] = np.absolute(self.scores_dict['test_score']).mean() \n",
    "            \n",
    "    \n",
    "    def select_best_algorithm(self):\n",
    "        '''select algorithm with lowest outer CV score'''\n",
    "        self.best_algorithm = min (self.mean_rmse, key=self.mean_rmse.get)\n",
    "        print ('\\nBest Performing Algorithm: ', self.best_algorithm.estimator)\n",
    "\n",
    "   \n",
    "    def tune_best_algorithm (self, feature_train, feature_test, target_train, target_test): \n",
    "        '''This function performs hyperparameter tuning on the whole training set with the best algorithm '''\n",
    "        \n",
    "        grid1 = { \n",
    "                'num_leaves': [6, 8, 20, 30],\n",
    "                'max_depth': [2, 4, 6, 8, 10],\n",
    "                'n_estimators': [50, 100, 200, 500],\n",
    "                'colsample_bytree': [0.3, 1.0]\n",
    "                }\n",
    "        \n",
    "        gcv_model_select = GridSearchCV(estimator=self.best_algorithm.estimator,\n",
    "                                        param_grid=grid1,\n",
    "                                        scoring='neg_root_mean_squared_error',\n",
    "                                        n_jobs=-1,\n",
    "                                        cv = 2,\n",
    "                                        verbose=0,\n",
    "                                        refit=True)\n",
    "\n",
    "        gcv_model_select.fit(feature_train, target_train)\n",
    "            \n",
    "        self.best_model = gcv_model_select.best_estimator_\n",
    "        self.best_score = gcv_model_select.best_score_\n",
    "        self.best_params = gcv_model_select.best_params_\n",
    "            \n",
    "        self.train_mae = mean_absolute_error(y_true=np.exp(target_train), y_pred=np.exp(self.best_model.predict(feature_train)))\n",
    "        self.test_mae  = mean_absolute_error(y_true=np.exp(target_test),  y_pred=np.exp(self.best_model.predict(feature_test)))\n",
    "\n",
    "        self.train_r2 = r2_score (y_true=np.exp(target_train), y_pred=np.exp(self.best_model.predict(feature_train)))\n",
    "        self.test_r2  = r2_score (y_true=np.exp(target_test),  y_pred=np.exp(self.best_model.predict(feature_test)))\n",
    "\n",
    "   \n",
    "\n",
    "    def best_model_predict(self, features):\n",
    "        '''scores features using best model'''\n",
    "        self.predictions = self.best_model.predict(features)\n",
    "       \n",
    "    \n",
    "    def save_results(self):\n",
    "        pass \n",
    "    \n",
    "        \n",
    "    @staticmethod\n",
    "    def get_feature_importance(model, cols):\n",
    "        '''retrieves and sorts feature importances'''\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            importances = model.feature_importances_\n",
    "            cols = model.feature_name_\n",
    "            feature_importances = pd.DataFrame({'feature':cols, 'importance':importances})\n",
    "            feature_importances.sort_values(by='importance', ascending=False, inplace=True)\n",
    "            #set index to 'feature'\n",
    "            feature_importances.set_index('feature', inplace=True, drop=True)\n",
    "            return feature_importances\n",
    "        else:\n",
    "            #some models don't have feature_importances_\n",
    "            return \"Feature importances do not exist for given model\"\n",
    "\n",
    "        \n",
    "    def print_summary(self):\n",
    "        '''prints summary of models, best model, and feature importance'''\n",
    "        print('\\nModel Summaries:\\n')\n",
    "        \n",
    "        print('Best Estimator:' ,self.best_model)\n",
    "        print('Best CV Score: %.2f' % np.abs(self.best_score))\n",
    "        print('Best Parameters: %s' % self.best_params)\n",
    "        \n",
    "        print('\\nTrain MAE: %.2f' % self.train_mae)\n",
    "        print(' Test MAE: %.2f' %  self.test_mae)\n",
    "\n",
    "        print('\\nTrain R2: %.2f' % self.train_r2)\n",
    "        print(' Test R2: %.2f' %   self.test_r2)\n",
    "            \n",
    "        feature_importances = self.get_feature_importance(self.best_model, data.feature_cols)\n",
    "        feature_importances[0:25].plot.bar(figsize=(20,10))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    \n",
    "    train_file = \"/Users/vusalbabashov/Desktop/house-prices/data/raw/train.csv\"\n",
    "    test_file =  \"/Users/vusalbabashov/Desktop/house-prices/data/raw/test.csv\"\n",
    "    \n",
    "    \n",
    "    nominal_cols = ['MSSubClass', 'MSZoning', 'Street', 'LandContour', 'LotConfig', \n",
    "                   'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle',\n",
    "                   'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', \n",
    "                   'Foundation', 'Heating', 'CentralAir', 'GarageType', 'MoSold',\n",
    "                   'SaleType', 'SaleCondition'] # removed Alley, MiscFeature, \n",
    "\n",
    "    ordinal_cols = ['LotShape', 'Utilities', 'LandSlope', 'OverallQual', 'OverallCond', \n",
    "                   'ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure', \n",
    "                   'BsmtFinType1', 'BsmtFinType2', 'HeatingQC', 'Electrical', 'KitchenQual', \n",
    "                   'Functional', 'FireplaceQu', 'GarageFinish', 'GarageQual', 'GarageCond',\n",
    "                   'PavedDrive'] #removed PoolQC, Fence,\n",
    "\n",
    "\n",
    "    numeric_cols = ['Id','LotFrontage','LotArea','YearBuilt','YearRemodAdd','MasVnrArea','BsmtFinSF1',\n",
    "                  'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF','1stFlrSF','2ndFlrSF','LowQualFinSF','GrLivArea',\n",
    "                  'BsmtFullBath','BsmtHalfBath','FullBath','HalfBath','BedroomAbvGr','KitchenAbvGr', 'TotRmsAbvGrd',\n",
    "                  'Fireplaces','GarageCars','GarageArea','WoodDeckSF','OpenPorchSF','EnclosedPorch',\n",
    "                   '3SsnPorch','ScreenPorch','PoolArea','MiscVal', 'GarageYrBlt', 'YrSold'] # removed the SalePrice\n",
    "\n",
    "    \n",
    "    target_col ='SalePrice'\n",
    "    \n",
    "    \n",
    "    lot_shape = ['IR3','IR2','IR1','Reg']\n",
    "    utilities = ['ELO', 'NoSeWa', 'NoSewr','AllPub']\n",
    "    land_slope = ['Sev','Mod','Gtl']\n",
    "    overall_qual = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]  \n",
    "    overall_cond = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]  \n",
    "    exter_qual = ['Po', 'Fa', 'TA', 'Gd', 'Ex']\n",
    "    exter_cond = ['Po', 'Fa', 'TA', 'Gd', 'Ex']\n",
    "    bsmt_qual  = ['NA', 'Po', 'Fa', 'TA', 'Gd', 'Ex']\n",
    "    bsmt_cond  = ['NA', 'Po', 'Fa', 'TA', 'Gd', 'Ex']\n",
    "    bsmt_exposure  = ['NA', 'No', 'Mn', 'Av', 'Gd']\n",
    "    bsmt_fin_type1 = ['NA', 'Unf', 'LwQ', 'Rec', 'BLQ', 'ALQ', 'GLQ']\n",
    "    bsmt_fin_type2 = ['NA', 'Unf', 'LwQ', 'Rec', 'BLQ', 'ALQ', 'GLQ']\n",
    "    heating_qual = ['Po', 'Fa', 'TA', 'Gd', 'Ex']\n",
    "    electrical = ['Mix', 'FuseP', 'FuseF', 'FuseA', 'SBrkr']\n",
    "    kitchen_qual = ['Po', 'Fa', 'TA', 'Gd', 'Ex']\n",
    "    functional = ['Sal', 'Sev', 'Maj2', 'Maj1', 'Mod', 'Min2', 'Min1', 'Typ']\n",
    "    fire_place_qual = ['NA', 'Po', 'Fa', 'TA', 'Gd', 'Ex']\n",
    "    garage_finish = ['NA', 'Unf', 'RFn', 'Fin']\n",
    "    garage_qual = ['NA', 'Po', 'Fa', 'TA', 'Gd', 'Ex']\n",
    "    garage_cond = ['NA', 'Po', 'Fa', 'TA', 'Gd', 'Ex']\n",
    "    paved_drive = ['N', 'P', 'Y']\n",
    "\n",
    "    ordinal_cat_list = [lot_shape , utilities, land_slope, overall_qual, overall_cond, exter_qual, exter_cond, bsmt_qual, \n",
    "                              bsmt_cond, bsmt_exposure, bsmt_fin_type1, bsmt_fin_type2, heating_qual, electrical, kitchen_qual,\n",
    "                              functional, fire_place_qual, garage_finish, garage_qual, garage_cond, paved_drive] \n",
    "    \n",
    "    \n",
    "    #turn feature engineering on/off\n",
    "    engineer_features = True\n",
    "    \n",
    "    #Create Data object\n",
    "    data = Data(train_file, test_file, nominal_cols, ordinal_cols, ordinal_cat_list, numeric_cols, target_col)\n",
    "    \n",
    "    \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(data.train_df.drop(['Id', 'SalePrice'], axis=1),data.train_df['SalePrice'],\n",
    "                                                             test_size=0.2,\n",
    "                                                             random_state=0)\n",
    "\n",
    "    X_features = data.test_df.drop(columns=['Id'], axis=1)\n",
    "    \n",
    "    \n",
    "    #Engineer features\n",
    "    if engineer_features:\n",
    "        feature_generator = FeatureGenerator(data)\n",
    "        feature_generator.create_pipeline(X_train, y_train)\n",
    "        X_train = feature_generator.create_features(X_train)\n",
    "        X_test  = feature_generator.create_features(X_test)\n",
    "        X_features = feature_generator.create_features(X_features)\n",
    "    \n",
    "       \n",
    "    #Create model container\n",
    "    models = ModelContainer()\n",
    "    models.nested_cross_validation(X_train, y_train)\n",
    "    models.select_best_algorithm()\n",
    "    models.tune_best_algorithm(X_train, X_test, y_train, y_test)\n",
    "    models.best_model_predict(X_features)\n",
    "    models.print_summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
