{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House Prices Prediction Model Training, Evaluation & Selection Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - DEFINE\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---- 1 Define the problem ----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, using the Kaggle [dataset](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data?select=train.csv), first, I'll establish simple baseline model using the OLS regerssion, and then I'll develop a few predictive models, namely, random forest, xgboost and lightgbm and compare the performance of these models against the baseline. \n",
    "\n",
    "### Objective: \n",
    "- To understand what factors contribute most to house prices.\n",
    "\n",
    "- To create a model that predicts the price of a house with a given several features. \n",
    "\n",
    "- To create or improve predictive accuracy of the model compared to baseline model\n",
    "\n",
    "The implementation of this model will allow housing agencies (e.g., CMHC), real-estate companies, banks, municipial governments and home buyers to make informed decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check versions of the Python and some key packages to ensure most recent version is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -a 'Vusal Babashov' -u -d -v -p numpy,mlxtend,matplotlib,sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda update scikit-learn numpy matplotlib "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, explained_variance_score \n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor\n",
    "import lightgbm as lgb \n",
    "import xgboost as xgb\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "\n",
    "#your info here\n",
    "__author__ = \"Vusal Babashov\"\n",
    "__email__ = \"vbabashov@gmail.com\"\n",
    "__website__ = 'https://vbabashov.github.io'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data into a Pandas dataframe\n",
    "file_path = \"/Users/vusalbabashov/Desktop/house-prices/data/\"\n",
    "df_train = pd.read_csv(file_path + \"train.csv\")\n",
    "df_test_feature = pd.read_csv(file_path + \"test.csv\")\n",
    "df_test_target = pd.read_csv(file_path + \"sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - DISCOVER\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps including Obtain data, Clean data, Explore data, and full EDA are implemented in a seperate [notebook](https://github.com/vbabashov/house-prices/blob/main/price_prediction_EDA.ipynb) due to size and readibility.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PreProcessing Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_missing_cols_df (df):\n",
    "    '''Drops the columns with 80% or hihgher missingness'''\n",
    "    missing_cols = []  \n",
    "    for col in df.columns:\n",
    "        if df[col].isnull().sum()/df.shape[0] >= 0.8:\n",
    "            missing_cols.append(col)\n",
    "    dropped_df=df.drop(columns=missing_cols)\n",
    "    return dropped_df, missing_cols  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_missing_val_df (df):\n",
    "    ''' Imputes the continious columns with mean and categorical columns (which has 80% missingness) with the most frequent value'''\n",
    "    imputer_con = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    imputer_cat = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "    for col in df.columns:\n",
    "        if df[col].isnull().sum() > 0:    \n",
    "            if df[col].dtype.name == 'object':\n",
    "                 df[col] = imputer_cat.fit_transform(df[col].values.reshape(-1,1))\n",
    "            else:            \n",
    "                 df[col] = imputer_con.fit_transform(df[col].values.reshape(-1,1))\n",
    "    return df      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data_types_df (df):\n",
    "    ''' converts the types of data columns into the appropriate type'''\n",
    "    for col in df.columns: \n",
    "        if df[col].dtype.name == 'object':\n",
    "             df[col]=df[col].astype('category')\n",
    "        else:\n",
    "             df[col]=df[col].astype('float')\n",
    "    df['MSSubClass'] = df['MSSubClass'].astype('category')\n",
    "    df['MoSold'] = df['MoSold'].astype('category')  \n",
    "    return df            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_df(df, target):\n",
    "    '''returns target dataframe'''\n",
    "    return df[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns_df (df, drop_cols):\n",
    "    '''drop the specified columns from the dataframe'''\n",
    "    df=df.drop (columns=drop_cols)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_of_cat_and_cont_vars (df):\n",
    "    '''determine the categorical and continous variables'''\n",
    "    categorical_cols = []\n",
    "    numerical_cols = []\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype.name == 'category':\n",
    "            categorical_cols.append(col)\n",
    "        else: \n",
    "            numerical_cols.append(col)\n",
    "    numerical_cols.remove('SalePrice')       \n",
    "    return categorical_cols, numerical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_feature_df(df, cat_vars, num_vars):\n",
    "    '''performs one-hot encoding on all categorical variables and combines result with continous variables'''\n",
    "    cat_df = pd.get_dummies(df[cat_vars])\n",
    "    num_df = df[num_vars].apply(pd.to_numeric)\n",
    "    return pd.concat([cat_df, num_df], axis=1)#,ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the missing columns and keep track of columns to be dropped\n",
    "dropped_train_df, missing_cols_train_df =  drop_missing_cols_df (df_train) \n",
    "dropped_test_df = drop_columns_df (df_test_feature, missing_cols_train_df) # drop the same columns from the test features to ensure consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#impute the missing values in a given row and col\n",
    "imputed_train_df = impute_missing_val_df (dropped_train_df)\n",
    "imputed_test_df = impute_missing_val_df (dropped_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the types to categorical and numeric columns to float\n",
    "type_converted_train_df = convert_data_types_df (imputed_train_df)\n",
    "type_converted_test_df = convert_data_types_df (imputed_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate and get the list of categorical and numerical features\n",
    "categorical_vars, numerical_vars = list_of_cat_and_cont_vars(type_converted_train_df)\n",
    "target_var = 'SalePrice'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode categorical data and get final feature dfs\n",
    "feature_train_df = one_hot_encode_feature_df(type_converted_train_df, categorical_vars, numerical_vars)\n",
    "feature_test_df  = one_hot_encode_feature_df(type_converted_test_df,  categorical_vars, numerical_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get target column from training and test dataframes\n",
    "target_train_df = get_target_df(df_train, target_var)\n",
    "target_test_df  = get_target_df(df_test_target, target_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_final_train, feature_final_test = feature_train_df.align(feature_test_df, join='inner', axis=1)  # inner join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---- Establish a baseline ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train RMSE for Baseline Model: 23481.28\n",
      "\n",
      " Test RMSE for Baseline Model: 75136.47\n"
     ]
    }
   ],
   "source": [
    "# Create linear regression object\n",
    "regr = LinearRegression()\n",
    "regr.fit(feature_final_train, target_train_df)\n",
    "y_pred = regr.predict(feature_final_test)\n",
    "y_pred_train = regr.predict(feature_final_train)\n",
    "print ('\\n Train RMSE for Baseline Model: %.2f'% mean_squared_error(target_train_df, y_pred_train, squared=False))\n",
    "print ('\\n Test RMSE for Baseline Model: %.2f'%  mean_squared_error(target_test_df, y_pred, squared=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---- Hypothesize solution ----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I conjecture that the RMSE can be improved by one of the following three popular tree-based ML methods. These methods have been shown to be succesful in competations like Kaggle. So let's try and compare their performance on housing dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ridge Regression\n",
    "- Lasso Regression\n",
    "- Random Forest\n",
    "- Xgboost \n",
    "- Lightgbm\n",
    "- HistGradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - DEVELOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the process, I'll look into creating features, tuning models, and training/validing models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---- Engineer features  ----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, I'll decide against engineering and adding any feature and proceed to training and testing the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---- Create models ----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll create and tune the models that I brainstormed eariler. Below, I'll show model selection and algorithm comparison using the nested-cross validation procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing Regressors\n",
    "reg1 = RandomForestRegressor(random_state=1)\n",
    "reg2 = XGBRegressor(random_state=1)\n",
    "reg3 = LGBMRegressor(random_state=1)\n",
    "reg4 = Lasso(random_state=1)\n",
    "reg5 = Ridge(random_state=1)\n",
    "reg6 = HistGradientBoostingRegressor(random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the parameter grids for hyperparameter tuning, i.e, Model Selection\n",
    "param_grid1 = {'n_estimators': [1000, 5000, 10000]}\n",
    "\n",
    "param_grid2 = {\n",
    "            'max_depth': [2,7,10],\n",
    "            'n_estimators': [50,100,150],\n",
    "            'learning_rate': [0.1, 0.01, 0.05]\n",
    "            }\n",
    "\n",
    "param_grid3 = {'num_leaves': [45, 60],\n",
    "              'feature_fraction': [0.1, 0.9],\n",
    "              'bagging_fraction': [0.8, 1],\n",
    "              'max_depth':[6,7,8],\n",
    "              'min_split_gain': [0.001, 0.1],\n",
    "              'min_child_weight':[2,3,4]\n",
    "              }\n",
    "\n",
    "param_grid4 = {'alpha':[0.02,0.03]}\n",
    "param_grid5 = {'alpha':[200,300, 500]}\n",
    "\n",
    "param_grid6={\n",
    "            'learning_rate': [0.1, 0.05, 0.02, 0.01], \n",
    "            'max_depth':[4,6], \n",
    "            'min_samples_leaf':[3,5,9,17], \n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up multiple GridSearchCV objects for model selection and algorithm comparison\n",
    "\n",
    "gridcvs = {}\n",
    "inner_cv = KFold(n_splits=2, shuffle=True, random_state=1)\n",
    "\n",
    "for pgrid, est, name in zip((param_grid1, param_grid2, param_grid3, param_grid4, param_grid5, param_grid6),\n",
    "                            (reg1, reg2, reg3, reg4, reg5, reg6),\n",
    "                            ('RForest', 'Xgboost', 'LightGBM', 'Lasso', 'Ridge', 'HistBasedGradBoosting')):\n",
    "    gcv = GridSearchCV(estimator=est,\n",
    "                       param_grid=pgrid,\n",
    "                       scoring = 'neg_root_mean_squared_error',\n",
    "                       n_jobs=-1,\n",
    "                       cv=inner_cv,\n",
    "                       verbose=0,\n",
    "                       refit=True)\n",
    "    gridcvs[name] = gcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------- \n",
      "\n",
      "Algorithm: HistBasedGradBoosting\n",
      "    Inner loop:\n",
      "\n",
      "      Best Score (avg. of inner test folds) -30690.84\n",
      "        Best parameters: HistGradientBoostingRegressor(max_depth=4, min_samples_leaf=17, random_state=1)\n",
      "        Score (on outer test fold) -29140.46\n",
      "\n",
      "      Best Score (avg. of inner test folds) -32231.72\n",
      "        Best parameters: HistGradientBoostingRegressor(max_depth=4, min_samples_leaf=17, random_state=1)\n",
      "        Score (on outer test fold) -30592.28\n",
      "\n",
      "      Best Score (avg. of inner test folds) -30505.01\n",
      "        Best parameters: HistGradientBoostingRegressor(max_depth=4, min_samples_leaf=9, random_state=1)\n",
      "        Score (on outer test fold) -26272.48\n",
      "\n",
      "      Best Score (avg. of inner test folds) -27035.99\n",
      "        Best parameters: HistGradientBoostingRegressor(learning_rate=0.05, max_depth=6,\n",
      "                              min_samples_leaf=17, random_state=1)\n",
      "        Score (on outer test fold) -35432.56\n",
      "\n",
      "      Best Score (avg. of inner test folds) -34550.21\n",
      "        Best parameters: HistGradientBoostingRegressor(learning_rate=0.05, max_depth=4,\n",
      "                              min_samples_leaf=17, random_state=1)\n",
      "        Score (on outer test fold) -22399.67\n",
      "\n",
      "HistBasedGradBoosting | outer test folds Ave. Score -28767.49 +/- 4351.23\n",
      "-------------------------------------------------- \n",
      "\n",
      "Algorithm: Lasso\n",
      "    Inner loop:\n",
      "\n",
      "      Best Score (avg. of inner test folds) -50185.11\n",
      "        Best parameters: Lasso(alpha=0.03, random_state=1)\n",
      "        Score (on outer test fold) -33669.37\n",
      "\n",
      "      Best Score (avg. of inner test folds) -36347.05\n",
      "        Best parameters: Lasso(alpha=0.03, random_state=1)\n",
      "        Score (on outer test fold) -55479.59\n",
      "\n",
      "      Best Score (avg. of inner test folds) -49201.56\n",
      "        Best parameters: Lasso(alpha=0.03, random_state=1)\n",
      "        Score (on outer test fold) -28324.21\n",
      "\n",
      "      Best Score (avg. of inner test folds) -43060.62\n",
      "        Best parameters: Lasso(alpha=0.03, random_state=1)\n",
      "        Score (on outer test fold) -44984.62\n",
      "\n",
      "      Best Score (avg. of inner test folds) -49201.89\n",
      "        Best parameters: Lasso(alpha=0.03, random_state=1)\n",
      "        Score (on outer test fold) -25054.14\n",
      "\n",
      "Lasso | outer test folds Ave. Score -37502.39 +/- 11252.14\n",
      "-------------------------------------------------- \n",
      "\n",
      "Algorithm: LightGBM\n",
      "    Inner loop:\n",
      "\n",
      "      Best Score (avg. of inner test folds) -30843.75\n",
      "        Best parameters: LGBMRegressor(bagging_fraction=0.8, feature_fraction=0.9, max_depth=6,\n",
      "              min_child_weight=2, min_split_gain=0.001, num_leaves=45,\n",
      "              random_state=1)\n",
      "        Score (on outer test fold) -28139.40\n",
      "\n",
      "      Best Score (avg. of inner test folds) -31405.34\n",
      "        Best parameters: LGBMRegressor(bagging_fraction=0.8, feature_fraction=0.9, max_depth=7,\n",
      "              min_child_weight=2, min_split_gain=0.001, num_leaves=45,\n",
      "              random_state=1)\n",
      "        Score (on outer test fold) -29748.98\n",
      "\n",
      "      Best Score (avg. of inner test folds) -30423.06\n",
      "        Best parameters: LGBMRegressor(bagging_fraction=0.8, feature_fraction=0.1, max_depth=7,\n",
      "              min_child_weight=2, min_split_gain=0.001, num_leaves=45,\n",
      "              random_state=1)\n",
      "        Score (on outer test fold) -27543.47\n",
      "\n",
      "      Best Score (avg. of inner test folds) -27313.98\n",
      "        Best parameters: LGBMRegressor(bagging_fraction=0.8, feature_fraction=0.9, max_depth=8,\n",
      "              min_child_weight=2, min_split_gain=0.001, num_leaves=45,\n",
      "              random_state=1)\n",
      "        Score (on outer test fold) -35381.07\n",
      "\n",
      "      Best Score (avg. of inner test folds) -34975.97\n",
      "        Best parameters: LGBMRegressor(bagging_fraction=0.8, feature_fraction=0.1, max_depth=6,\n",
      "              min_child_weight=2, min_split_gain=0.001, num_leaves=45,\n",
      "              random_state=1)\n",
      "        Score (on outer test fold) -23815.66\n",
      "\n",
      "LightGBM | outer test folds Ave. Score -28925.72 +/- 3768.05\n",
      "-------------------------------------------------- \n",
      "\n",
      "Algorithm: RForest\n",
      "    Inner loop:\n",
      "\n",
      "      Best Score (avg. of inner test folds) -32890.42\n",
      "        Best parameters: RandomForestRegressor(n_estimators=1000, random_state=1)\n",
      "        Score (on outer test fold) -27577.32\n",
      "\n",
      "      Best Score (avg. of inner test folds) -33720.93\n",
      "        Best parameters: RandomForestRegressor(n_estimators=5000, random_state=1)\n",
      "        Score (on outer test fold) -31193.14\n",
      "\n",
      "      Best Score (avg. of inner test folds) -32471.34\n",
      "        Best parameters: RandomForestRegressor(n_estimators=5000, random_state=1)\n",
      "        Score (on outer test fold) -28957.36\n",
      "\n",
      "      Best Score (avg. of inner test folds) -28909.47\n",
      "        Best parameters: RandomForestRegressor(n_estimators=10000, random_state=1)\n",
      "        Score (on outer test fold) -35710.99\n",
      "\n",
      "      Best Score (avg. of inner test folds) -37908.53\n",
      "        Best parameters: RandomForestRegressor(n_estimators=1000, random_state=1)\n",
      "        Score (on outer test fold) -23055.86\n",
      "\n",
      "RForest | outer test folds Ave. Score -29298.94 +/- 4165.55\n",
      "-------------------------------------------------- \n",
      "\n",
      "Algorithm: Ridge\n",
      "    Inner loop:\n",
      "\n",
      "      Best Score (avg. of inner test folds) -41177.03\n",
      "        Best parameters: Ridge(alpha=200, random_state=1)\n",
      "        Score (on outer test fold) -33674.35\n",
      "\n",
      "      Best Score (avg. of inner test folds) -33798.45\n",
      "        Best parameters: Ridge(alpha=200, random_state=1)\n",
      "        Score (on outer test fold) -47659.28\n",
      "\n",
      "      Best Score (avg. of inner test folds) -36449.93\n",
      "        Best parameters: Ridge(alpha=200, random_state=1)\n",
      "        Score (on outer test fold) -27660.46\n",
      "\n",
      "      Best Score (avg. of inner test folds) -35454.59\n",
      "        Best parameters: Ridge(alpha=200, random_state=1)\n",
      "        Score (on outer test fold) -38998.90\n",
      "\n",
      "      Best Score (avg. of inner test folds) -44931.44\n",
      "        Best parameters: Ridge(alpha=200, random_state=1)\n",
      "        Score (on outer test fold) -24424.09\n",
      "\n",
      "Ridge | outer test folds Ave. Score -34483.42 +/- 8275.15\n",
      "-------------------------------------------------- \n",
      "\n",
      "Algorithm: Xgboost\n",
      "    Inner loop:\n",
      "\n",
      "      Best Score (avg. of inner test folds) -31551.17\n",
      "        Best parameters: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "             importance_type='gain', interaction_constraints='',\n",
      "             learning_rate=0.05, max_delta_step=0, max_depth=7,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=150, n_jobs=8, num_parallel_tree=1, random_state=1,\n",
      "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "             tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "        Score (on outer test fold) -23967.28\n",
      "\n",
      "      Best Score (avg. of inner test folds) -31472.98\n",
      "        Best parameters: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "             importance_type='gain', interaction_constraints='',\n",
      "             learning_rate=0.1, max_delta_step=0, max_depth=2,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=150, n_jobs=8, num_parallel_tree=1, random_state=1,\n",
      "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "             tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "        Score (on outer test fold) -28500.72\n",
      "\n",
      "      Best Score (avg. of inner test folds) -28199.71\n",
      "        Best parameters: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "             importance_type='gain', interaction_constraints='',\n",
      "             learning_rate=0.1, max_delta_step=0, max_depth=2,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=150, n_jobs=8, num_parallel_tree=1, random_state=1,\n",
      "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "             tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "        Score (on outer test fold) -28156.61\n",
      "\n",
      "      Best Score (avg. of inner test folds) -27437.23\n",
      "        Best parameters: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "             importance_type='gain', interaction_constraints='',\n",
      "             learning_rate=0.1, max_delta_step=0, max_depth=2,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=150, n_jobs=8, num_parallel_tree=1, random_state=1,\n",
      "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "             tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "        Score (on outer test fold) -36147.43\n",
      "\n",
      "      Best Score (avg. of inner test folds) -37652.22\n",
      "        Best parameters: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "             importance_type='gain', interaction_constraints='',\n",
      "             learning_rate=0.1, max_delta_step=0, max_depth=2,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=150, n_jobs=8, num_parallel_tree=1, random_state=1,\n",
      "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "             tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "        Score (on outer test fold) -21183.09\n",
      "\n",
      "Xgboost | outer test folds Ave. Score -27591.02 +/- 5070.55\n"
     ]
    }
   ],
   "source": [
    "outer_cv = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "for name, gs_est in sorted(gridcvs.items()):\n",
    "    scores_dict = cross_validate(gs_est, \n",
    "                                 X=feature_final_train, \n",
    "                                 y=target_train_df, \n",
    "                                 cv=outer_cv,\n",
    "                                 return_estimator=True,\n",
    "                                 n_jobs=-1)\n",
    "\n",
    "    print(50 * '-', '\\n')\n",
    "    print('Algorithm:', name)\n",
    "    print('    Inner loop:')\n",
    "    \n",
    "    \n",
    "    for i in range(scores_dict['test_score'].shape[0]):\n",
    "\n",
    "        print('\\n      Best Score (avg. of inner test folds) %.2f' % (scores_dict['estimator'][i].best_score_))\n",
    "        print('        Best parameters:', scores_dict['estimator'][i].best_estimator_)\n",
    "        print('        Score (on outer test fold) %.2f' % (scores_dict['test_score'][i]))\n",
    "\n",
    "    print('\\n%s | outer test folds Ave. Score %.2f +/- %.2f' % \n",
    "          (name, scores_dict['test_score'].mean(), \n",
    "           scores_dict['test_score'].std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---- 9 Test models ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 144 candidates, totalling 720 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    5.0s\n",
      "[Parallel(n_jobs=-1)]: Done 652 tasks      | elapsed:   10.7s\n",
      "[Parallel(n_jobs=-1)]: Done 705 out of 720 | elapsed:   11.5s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 720 out of 720 | elapsed:   11.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=LGBMRegressor(random_state=1), n_jobs=-1,\n",
       "             param_grid={'bagging_fraction': [0.8, 1],\n",
       "                         'feature_fraction': [0.1, 0.9], 'max_depth': [6, 7, 8],\n",
       "                         'min_child_weight': [2, 3, 4],\n",
       "                         'min_split_gain': [0.001, 0.1],\n",
       "                         'num_leaves': [45, 60]},\n",
       "             scoring='neg_root_mean_squared_error', verbose=1)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#5-fold cross validation on the best model and measure RMSE\n",
    "gcv_model_select = GridSearchCV(estimator=reg3,\n",
    "                                param_grid=param_grid3,\n",
    "                                scoring='neg_root_mean_squared_error',\n",
    "                                n_jobs=-1,\n",
    "                                cv = 5,\n",
    "                                verbose=1,\n",
    "                                refit=True)\n",
    "\n",
    "gcv_model_select.fit(feature_final_train, target_train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---- 10 Select best model  ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE score: -28070.74 (average over k-fold CV test folds)\n",
      "Best Parameters: {'bagging_fraction': 0.8, 'feature_fraction': 0.9, 'max_depth': 6, 'min_child_weight': 2, 'min_split_gain': 0.001, 'num_leaves': 45}\n",
      "\n",
      "Training RMSE for Best Model: 14228.79\n",
      "Test RMSE for Best Model: 73918.57\n"
     ]
    }
   ],
   "source": [
    "#select the model with the lowest error as your \"production\" model\n",
    "best_model = gcv_model_select.best_estimator_\n",
    "\n",
    "train_RMSE = mean_squared_error(y_true=target_train_df, y_pred=best_model.predict(feature_final_train), squared=False)\n",
    "test_RMSE  = mean_squared_error(y_true=target_test_df,  y_pred=best_model.predict(feature_final_test),  squared=False)\n",
    "\n",
    "print('RMSE score: %.2f (average over k-fold CV test folds)' %\n",
    "      (gcv_model_select.best_score_))\n",
    "print('Best Parameters: %s' % gcv_model_select.best_params_)\n",
    "\n",
    "print('\\nTraining RMSE for Best Model: %.2f' % (train_RMSE))\n",
    "print('Test RMSE for Best Model: %.2f' % (test_RMSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are signs of overfitting given the spread between training and test RMSE values.\n",
    "- With these results, there seems to be only a 4% (marginal) improvement in RMSE comparing the model performance between the best model and baseline model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 - DEPLOY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---- 11 Automate pipeline ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write script that trains model on entire training set, saves model to disk,\n",
    "#and scores the \"test\" dataset\n",
    "\n",
    "# Preprocessing for numerical data\n",
    "numerical_transformer = SimpleImputer(strategy='constant')\n",
    "\n",
    "# Preprocessing for categorical data\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "# Preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "        transformers =[\n",
    "            ('num', numerical_transformer, numerical_cols),\n",
    "            ('cat', categorical_transformer, categorical_cols)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---- 12 Deploy solution ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save your prediction to a csv file or optionally save them as a table in a SQL database\n",
    "#additionally, you want to save a visualization and summary of your prediction and feature importances\n",
    "#these visualizations and summaries will be extremely useful to business stakeholders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---- 13 Measure efficacy ----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll skip this step since we don't have the outcomes for the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
