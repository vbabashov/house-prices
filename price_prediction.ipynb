{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House Prices Prediction Model Training, Evaluation & Selection Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - DEFINE\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---- 1 Define the problem ----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, using the Kaggle [dataset](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data?select=train.csv), first, I'll establish simple baseline model using the OLS regerssion, and then I'll develop a few predictive models, namely, random forest, xgboost and lightgbm and compare the performance of these models against the baseline \n",
    "\n",
    "### Objective: \n",
    "- To understand what factors contribute most to house prices.\n",
    "\n",
    "- To create a model that predicts the price of a house with a given several features. \n",
    "\n",
    "- To create or improve predictive accuracy of the model compared to baseline model\n",
    "\n",
    "The implementation of this model will allow housing agencies (e.g., CMHC), real-estate companies, banks, municipial governments and home buyers to make informed decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check versions of the Python and some key packages to ensure most recent version is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -a 'Vusal Babashov' -u -d -v -p numpy,mlxtend,matplotlib,sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda update scikit-learn numpy matplotlib "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, explained_variance_score \n",
    "#from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor\n",
    "import lightgbm as lgb \n",
    "import xgboost as xgb\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "\n",
    "#your info here\n",
    "__author__ = \"Vusal Babashov\"\n",
    "__email__ = \"vbabashov@gmail.com\"\n",
    "__website__ = 'https://vbabashov.github.io'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data into a Pandas dataframe\n",
    "file_path = \"/Users/vusalbabashov/Desktop/house-prices/data/\"\n",
    "df_train = pd.read_csv(file_path + \"train.csv\")\n",
    "df_test_feature = pd.read_csv(file_path + \"test.csv\")\n",
    "df_test_target = pd.read_csv(file_path + \"sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - DISCOVER\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps including Obtain data, Clean data, Explore data, and full EDA are implemented in a seperate [notebook](https://github.com/vbabashov/house-prices/blob/main/price_prediction_EDA.ipynb) due to size and readibility.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PreProcessing Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_missing_cols_df (df):\n",
    "    '''Drops the columns with 80% or hihgher missingness'''\n",
    "    missing_cols = []  \n",
    "    for col in df.columns:\n",
    "        if df[col].isnull().sum()/df.shape[0] >= 0.8:\n",
    "            missing_cols.append(col)\n",
    "    dropped_df=df.drop(columns=missing_cols)\n",
    "    return dropped_df, missing_cols  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_missing_val_df (df):\n",
    "    ''' Imputes the continious columns with mean and categorical columns (which has 80% missingness) with the most frequent value'''\n",
    "    imputer_con = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    imputer_cat = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "    for col in df.columns:\n",
    "        if df[col].isnull().sum() > 0:    \n",
    "            if df[col].dtype.name == 'object':\n",
    "                 df[col] = imputer_cat.fit_transform(df[col].values.reshape(-1,1))\n",
    "            else:            \n",
    "                 df[col] = imputer_con.fit_transform(df[col].values.reshape(-1,1))\n",
    "    return df      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data_types_df (df):\n",
    "    ''' converts the types of data columns into the appropriate type'''\n",
    "    for col in df.columns: \n",
    "        if df[col].dtype.name == 'object':\n",
    "             df[col]=df[col].astype('category')\n",
    "        else:\n",
    "             df[col]=df[col].astype('float')\n",
    "    df['MSSubClass'] = df['MSSubClass'].astype('category')\n",
    "    df['MoSold'] = df['MoSold'].astype('category')  \n",
    "    return df            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_df(df, target):\n",
    "    '''returns target dataframe'''\n",
    "    return df[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns_df (df, drop_cols):\n",
    "    '''drop the specified columns from the dataframe'''\n",
    "    df=df.drop (columns=drop_cols)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_of_cat_and_cont_vars (df):\n",
    "    '''determine the categorical and continous variables'''\n",
    "    categorical_cols = []\n",
    "    numerical_cols = []\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype.name == 'category':\n",
    "            categorical_cols.append(col)\n",
    "        else: \n",
    "            numerical_cols.append(col)\n",
    "    numerical_cols.remove('SalePrice')       \n",
    "    return categorical_cols, numerical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_feature_df(df, cat_vars, num_vars):\n",
    "    '''performs one-hot encoding on all categorical variables and combines result with continous variables'''\n",
    "    cat_df = pd.get_dummies(df[cat_vars])\n",
    "    num_df = df[num_vars].apply(pd.to_numeric)\n",
    "    return pd.concat([cat_df, num_df], axis=1)#,ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the missing columns and keep track of columns to be dropped\n",
    "dropped_train_df, missing_cols_train_df =  drop_missing_cols_df (df_train) \n",
    "dropped_test_df = drop_columns_df (df_test_feature, missing_cols_train_df) # drop the same columns from the test features to ensure consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#impute the missing values in a given row and col\n",
    "imputed_train_df = impute_missing_val_df (dropped_train_df)\n",
    "imputed_test_df = impute_missing_val_df (dropped_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the types to categorical and numeric columns to float\n",
    "type_converted_train_df = convert_data_types_df (imputed_train_df)\n",
    "type_converted_test_df = convert_data_types_df (imputed_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate and get the list of categorical and numerical features\n",
    "categorical_vars, numerical_vars = list_of_cat_and_cont_vars(type_converted_train_df)\n",
    "target_var = 'SalePrice'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode categorical data and get final feature dfs\n",
    "feature_train_df = one_hot_encode_feature_df(type_converted_train_df, categorical_vars, numerical_vars)\n",
    "feature_test_df  = one_hot_encode_feature_df(type_converted_test_df,  categorical_vars, numerical_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get target column from training and test dataframes\n",
    "target_train_df = get_target_df(df_train, target_var)\n",
    "target_test_df  = get_target_df(df_test_target, target_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_final_train, feature_final_test = feature_train_df.align(feature_test_df, join='inner', axis=1)  # inner join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---- Establish a baseline ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " MAE for OLS: 56469.50\n",
      "\n",
      " RMSE for OLS: 75136.47\n"
     ]
    }
   ],
   "source": [
    "# Create linear regression object\n",
    "regr = LinearRegression()\n",
    "regr.fit(feature_final_train, target_train_df)\n",
    "y_pred = regr.predict(feature_final_test)\n",
    "print ('\\n MAE for OLS: %.2f' % mean_absolute_error(target_test_df, y_pred))\n",
    "print ('\\n RMSE for OLS: %.2f' % np.sqrt(mean_squared_error(target_test_df, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---- Hypothesize solution ----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I believe the MAE can be improved by one of the following three popular tree-based ML methods. These methods have been shown to be succesful in competations like Kaggle. So let's try and compare their performance on housing dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Random Forest\n",
    "- Xgboost \n",
    "- Lightgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - DEVELOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the process, I'll look into creating features, tuning models, and training/validing models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---- Engineer features  ----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, I'll decide against engineering and adding any feature and proceed to training and testing the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---- Create models ----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll create and tune the models that I brainstormed eariler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing Regressors\n",
    "reg1 = RandomForestRegressor(random_state=1)\n",
    "reg2 = XGBRegressor(random_state=1)\n",
    "reg3 = LGBMRegressor(random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the parameter grids\n",
    "param_grid1 = { 'n_estimators': [1000, 5000, 10000, 25000, 50000]}\n",
    "\n",
    "param_grid2 = {\n",
    "            'max_depth': range (2, 10, 1),\n",
    "            'n_estimators': range(60, 220, 40),\n",
    "            'learning_rate': [0.1, 0.01, 0.05]\n",
    "        }\n",
    "\n",
    "param_grid3 = {'num_leaves': (45, 60),\n",
    "              'feature_fraction': (0.1, 0.9),\n",
    "              'bagging_fraction': (0.8, 1),\n",
    "              'max_depth': (9, 13),\n",
    "              'min_split_gain': (0.001, 0.1),\n",
    "              'min_child_weight': (30, 50)\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up multiple GridSearchCV objects, 1 for each algorithm\n",
    "gridcvs = {}\n",
    "inner_cv = KFold(n_splits=2, shuffle=True, random_state=1)\n",
    "\n",
    "for pgrid, est, name in zip((param_grid1, param_grid2, param_grid3),\n",
    "                            (reg1, reg2, reg3),\n",
    "                            ('RForest', 'Xgboost', 'LightGBM')):\n",
    "    gcv = GridSearchCV(estimator=est,\n",
    "                       param_grid=pgrid,\n",
    "                       n_jobs=-1,\n",
    "                       cv=inner_cv,\n",
    "                       verbose=0,\n",
    "                       refit=True)\n",
    "    gridcvs[name] = gcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------- \n",
      "\n",
      "Algorithm: LightGBM\n",
      "    Inner loop:\n",
      "\n",
      "        Best Score (avg. of inner test folds) 83.18%\n",
      "        Best parameters: LGBMRegressor(bagging_fraction=0.8, feature_fraction=0.9, max_depth=9,\n",
      "              min_child_weight=30, min_split_gain=0.001, num_leaves=45,\n",
      "              random_state=1)\n",
      "        Score (on outer test fold) 88.05%\n",
      "\n",
      "        Best Score (avg. of inner test folds) 83.85%\n",
      "        Best parameters: LGBMRegressor(bagging_fraction=0.8, feature_fraction=0.9, max_depth=13,\n",
      "              min_child_weight=30, min_split_gain=0.001, num_leaves=45,\n",
      "              random_state=1)\n",
      "        Score (on outer test fold) 84.59%\n",
      "\n",
      "        Best Score (avg. of inner test folds) 85.26%\n",
      "        Best parameters: LGBMRegressor(bagging_fraction=0.8, feature_fraction=0.9, max_depth=9,\n",
      "              min_child_weight=30, min_split_gain=0.001, num_leaves=45,\n",
      "              random_state=1)\n",
      "        Score (on outer test fold) 88.01%\n",
      "\n",
      "        Best Score (avg. of inner test folds) 87.50%\n",
      "        Best parameters: LGBMRegressor(bagging_fraction=0.8, feature_fraction=0.9, max_depth=13,\n",
      "              min_child_weight=30, min_split_gain=0.001, num_leaves=45,\n",
      "              random_state=1)\n",
      "        Score (on outer test fold) 81.82%\n",
      "\n",
      "        Best Score (avg. of inner test folds) 80.73%\n",
      "        Best parameters: LGBMRegressor(bagging_fraction=0.8, feature_fraction=0.1, max_depth=13,\n",
      "              min_child_weight=30, min_split_gain=0.001, num_leaves=45,\n",
      "              random_state=1)\n",
      "        Score (on outer test fold) 90.54%\n",
      "\n",
      "LightGBM | outer Score 86.60% +/- 3.05\n",
      "-------------------------------------------------- \n",
      "\n",
      "Algorithm: RForest\n",
      "    Inner loop:\n",
      "\n",
      "        Best Score (avg. of inner test folds) 82.03%\n",
      "        Best parameters: RandomForestRegressor(n_estimators=1000, random_state=1)\n",
      "        Score (on outer test fold) 89.34%\n",
      "\n",
      "        Best Score (avg. of inner test folds) 82.03%\n",
      "        Best parameters: RandomForestRegressor(n_estimators=5000, random_state=1)\n",
      "        Score (on outer test fold) 83.97%\n",
      "\n",
      "        Best Score (avg. of inner test folds) 83.51%\n",
      "        Best parameters: RandomForestRegressor(n_estimators=500, random_state=1)\n",
      "        Score (on outer test fold) 85.54%\n",
      "\n",
      "        Best Score (avg. of inner test folds) 86.44%\n",
      "        Best parameters: RandomForestRegressor(n_estimators=10000, random_state=1)\n",
      "        Score (on outer test fold) 81.31%\n",
      "\n",
      "        Best Score (avg. of inner test folds) 77.60%\n",
      "        Best parameters: RandomForestRegressor(n_estimators=1000, random_state=1)\n",
      "        Score (on outer test fold) 90.54%\n",
      "\n",
      "RForest | outer Score 86.14% +/- 3.41\n",
      "-------------------------------------------------- \n",
      "\n",
      "Algorithm: XGBOOST\n",
      "    Inner loop:\n",
      "\n",
      "        Best Score (avg. of inner test folds) 83.61%\n",
      "        Best parameters: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "             importance_type='gain', interaction_constraints='',\n",
      "             learning_rate=0.05, max_delta_step=0, max_depth=6,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=180, n_jobs=8, num_parallel_tree=1, random_state=1,\n",
      "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "             tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "        Score (on outer test fold) 92.63%\n",
      "\n",
      "        Best Score (avg. of inner test folds) 84.22%\n",
      "        Best parameters: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "             importance_type='gain', interaction_constraints='',\n",
      "             learning_rate=0.1, max_delta_step=0, max_depth=2,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=180, n_jobs=8, num_parallel_tree=1, random_state=1,\n",
      "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "             tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "        Score (on outer test fold) 86.98%\n",
      "\n",
      "        Best Score (avg. of inner test folds) 87.79%\n",
      "        Best parameters: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "             importance_type='gain', interaction_constraints='',\n",
      "             learning_rate=0.1, max_delta_step=0, max_depth=2,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=180, n_jobs=8, num_parallel_tree=1, random_state=1,\n",
      "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "             tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "        Score (on outer test fold) 86.76%\n",
      "\n",
      "        Best Score (avg. of inner test folds) 88.41%\n",
      "        Best parameters: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "             importance_type='gain', interaction_constraints='',\n",
      "             learning_rate=0.1, max_delta_step=0, max_depth=4,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=180, n_jobs=8, num_parallel_tree=1, random_state=1,\n",
      "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "             tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "        Score (on outer test fold) 82.17%\n",
      "\n",
      "        Best Score (avg. of inner test folds) 78.39%\n",
      "        Best parameters: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "             importance_type='gain', interaction_constraints='',\n",
      "             learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=140, n_jobs=8, num_parallel_tree=1, random_state=1,\n",
      "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "             tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "        Score (on outer test fold) 92.54%\n",
      "\n",
      "XGBOOST | outer Score 88.22% +/- 3.96\n"
     ]
    }
   ],
   "source": [
    "outer_cv = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "for name, gs_est in sorted(gridcvs.items()):\n",
    "    scores_dict = cross_validate(gs_est, \n",
    "                                 X=feature_final_train, \n",
    "                                 y=target_train_df, \n",
    "                                 cv=outer_cv,\n",
    "                                 return_estimator=True,\n",
    "                                 n_jobs=-1)\n",
    "\n",
    "    print(50 * '-', '\\n')\n",
    "    print('Algorithm:', name)\n",
    "    print('    Inner loop:')\n",
    "    \n",
    "    \n",
    "    for i in range(scores_dict['test_score'].shape[0]):\n",
    "\n",
    "        print('\\n        Best Score (avg. of inner test folds) %.2f%%' % (scores_dict['estimator'][i].best_score_ * 100))\n",
    "        print('        Best parameters:', scores_dict['estimator'][i].best_estimator_)\n",
    "        print('        Score (on outer test fold) %.2f%%' % (scores_dict['test_score'][i]*100))\n",
    "\n",
    "    print('\\n%s | outer Score %.2f%% +/- %.2f' % \n",
    "          (name, scores_dict['test_score'].mean() * 100, \n",
    "           scores_dict['test_score'].std() * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---- 9 Test models ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 5 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:  2.9min remaining:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed: 27.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=2, random_state=1, shuffle=True),\n",
       "             estimator=RandomForestRegressor(random_state=1), n_jobs=-1,\n",
       "             param_grid={'n_estimators': [1000, 5000, 10000, 25000, 50000]},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#do 2-fold cross validation on the best model and measure RMSE\n",
    "gcv_model_select = GridSearchCV(estimator=reg1,\n",
    "                                param_grid=param_grid1,\n",
    "                               # scoring='r2',\n",
    "                                n_jobs=-1,\n",
    "                                cv= inner_cv,\n",
    "                                verbose=1,\n",
    "                                refit=True)\n",
    "\n",
    "gcv_model_select.fit(feature_final_train, target_train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---- 10 Select best model  ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score: 85.73% (average over k-fold CV test folds)\n",
      "Best Parameters: {'n_estimators': 10000}\n",
      "Training RMSE: 10769.24\n",
      "Test RMSE: 70017.55\n"
     ]
    }
   ],
   "source": [
    "#select the model with the lowest error as your \"prodcuction\" model\n",
    "best_model = gcv_model_select.best_estimator_\n",
    "\n",
    "\n",
    "train_RMSE = np.sqrt(mean_squared_error(y_true=target_train_df, y_pred=best_model.predict(feature_final_train)))\n",
    "test_RMSE  = np.sqrt(mean_squared_error(y_true=target_test_df,  y_pred=best_model.predict(feature_final_test)))\n",
    "\n",
    "print('R2 score: %.2f%% (average over k-fold CV test folds)' %\n",
    "      (100 * gcv_model_select.best_score_))\n",
    "print('Best Parameters: %s' % gcv_model_select.best_params_)\n",
    "\n",
    "print('Training RMSE: %.2f' % (train_RMSE))\n",
    "print('Test RMSE: %.2f' % (test_RMSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 - DEPLOY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---- 11 Automate pipeline ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write script that trains model on entire training set, saves model to disk,\n",
    "#and scores the \"test\" dataset\n",
    "\n",
    "# Preprocessing for numerical data\n",
    "numerical_transformer = SimpleImputer(strategy='constant')\n",
    "\n",
    "# Preprocessing for categorical data\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "# Preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "        transformers =[\n",
    "            ('num', numerical_transformer, numerical_cols),\n",
    "            ('cat', categorical_transformer, categorical_cols)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---- 12 Deploy solution ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save your prediction to a csv file or optionally save them as a table in a SQL database\n",
    "#additionally, you want to save a visualization and summary of your prediction and feature importances\n",
    "#these visualizations and summaries will be extremely useful to business stakeholders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---- 13 Measure efficacy ----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll skip this step since we don't have the outcomes for the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
